{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fuzzy-model",
   "metadata": {},
   "source": [
    "### Overview \n",
    "\n",
    "Inference speed of huge models is an issue in deep learning. There are many ways to boost the inference speed. This notebook discusses few of the vast possibilites to increase inference speed.\n",
    "\n",
    "There are primarily three methods as per the survey to increase inference speed namely :\n",
    "\n",
    "1. **Quantization**\n",
    "2. **ONXX runtime**\n",
    "3. **Pruning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-details",
   "metadata": {},
   "source": [
    "##### 1.Quantization\n",
    "\n",
    "1. Quantization refers to techniques for performing computations and storing tensors at lower bitwidths than floating point precision.\n",
    "\n",
    "2. A quantized model executes some or all of the operations on tensors with integers rather than floating point values.\n",
    "\n",
    "3. This allows for a more compact model representation and the use of high performance vectorized operations on many hardware platforms. \n",
    "\n",
    "4. `PyTorch supports INT8 quantization compared to typical FP32 models allowing for a 4x reduction in the model size and a 4x reduction in memory bandwidth requirements.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-hamilton",
   "metadata": {},
   "source": [
    "#### 2.ONNX Runtime\n",
    "\n",
    "The `open neural network exchange (onnx)` runtime provides an easy way to run machine learned models with high performance on CPU or GPU without dependencies on the training framework. Machine learning frameworks are usually optimized for batch training rather than for prediction, which is a more common scenario in applications, sites, and services. At a high level, you can:\n",
    "\n",
    "1. Train a model using your favorite framework.\n",
    "\n",
    "2. Convert or export the model into ONNX format.\n",
    "\n",
    "3. Load and run the model using ONNX Runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-inquiry",
   "metadata": {},
   "source": [
    "#### 3.Pruning\n",
    "\n",
    "1. If we take an insider look, there are few neurons that do not activate at all and that means they don’t impact output at all. On the other hand, few neurons’ activation strength could be very low and hence has a very small impact on the final output.\n",
    "\n",
    "2. There is a way where we decide weight thresholds, and if weight associated with a neuron falls in that range, we simply, prune that neuron.\n",
    "\n",
    "3. To prune a neuron, we need to set incoming and outgoing weights as zero for that neuron. As incoming and outgoing weights are zero, that neuron won’t have any impact on the final output.`pruning will make the model sparse which is easier to compress than a dense model.`\n",
    "\n",
    "4. `After pruning, it’s advisable to fine-tune the neural network a bit before using it in production, particularly when the threshold range is slightly higher and we cared more about removing more weights.`\n",
    "\n",
    "5. Pruning would slightly decrease performance but will increase inference latency. `According to the Deep Compression research,  pruning can decrease the number of effective neurons by 9 to 13 times.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-whale",
   "metadata": {},
   "source": [
    "#### Quick Demo\n",
    "\n",
    "We will demonstrate inference speed boost taking a pretrained hugging face model with quantization and onnx runtime\n",
    "\n",
    "1. ONNX runtime\n",
    "2. quantization\n",
    "3. quantization+onnx runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-youth",
   "metadata": {},
   "source": [
    "##### ONNX runtime\n",
    "1. Conversion of models to onnx format\n",
    "2. Inference on onnx runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-cinema",
   "metadata": {},
   "source": [
    "##### Loading necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-immigration",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from transformers.convert_graph_to_onnx import convert\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering,AutoModel\n",
    "import onnxruntime as ort\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-biography",
   "metadata": {},
   "source": [
    "#### Creating ONNX format of both long former and pubmedbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-personal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading tokenizer of longformer model and biomedical pubmed bert.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/longformer-base-4096-finetuned-squadv2\")\n",
    "tokenizer_pubmed = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")\n",
    "#Converting the models to onxx format to speed up model inference.\n",
    "#1.longformer 2.Pubmedbert\n",
    "convert(framework=\"pt\", tokenizer=tokenizer,model='mrm8488/longformer-base-4096-finetuned-squadv2',\n",
    "        output=Path(\"path/longformer.onnx\"), opset=12)\n",
    "convert(framework=\"pt\", tokenizer=tokenizer_pubmed,model='microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract',\n",
    "        output=Path(\"path/pubmedbert.onnx\"), opset=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-optimum",
   "metadata": {},
   "source": [
    "##### Quantizing the onnx models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-above",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.convert_graph_to_onnx import quantize\n",
    "quantize(Path('path/longformer.onnx'))\n",
    "quantize(Path('path/pubmedbert.onnx'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-filename",
   "metadata": {},
   "source": [
    "#### Inference Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = ort.InferenceSession('pubmedbert.onnx')\n",
    "session_quantized = ort.InferenceSession(\"pubmed-quantized.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-helena",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpus_creation(\"data_path\")\n",
    "encoded_input = tokenizer_pubmed(corpus, padding=True, truncation=True, max_length=500, return_tensors='pt')\n",
    "encoded_inputs_onnx = {k: v.cpu().detach().numpy() for k, v in encoded_input.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-supervisor",
   "metadata": {},
   "source": [
    "#### Action Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-cycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = datetime.now()\n",
    "sequence, pooled = session.run(None, encoded_inputs_onnx)\n",
    "t4 = datetime.now()\n",
    "print(\"model with onnx runntime : \",(t4-t3).total_seconds())\n",
    "\n",
    "t5 = datetime.now()\n",
    "sequence, pooled = session_quantized.run(None, encoded_inputs_onnx)\n",
    "t6 = datetime.now()\n",
    "print(\"model with onnx runntime and quantization enabled : \",(t6-t5).total_seconds())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-force",
   "metadata": {},
   "source": [
    "#### How about space occupied by the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-display",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def convert_bytes(num):\n",
    "    \"\"\"\n",
    "    this function will convert bytes to MB.... GB... etc\n",
    "    \"\"\"\n",
    "    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if num < 1024.0:\n",
    "            return \"%3.1f %s\" % (num, x)\n",
    "        num /= 1024.0\n",
    "def file_size(file_path):\n",
    "    \"\"\"\n",
    "    this function will return the file size\n",
    "    \"\"\"\n",
    "    if os.path.isfile(file_path):\n",
    "        file_info = os.stat(file_path)\n",
    "        return convert_bytes(file_info.st_size)\n",
    "\n",
    "print(\"onxx model size : \",file_size(\"pubmed.onnx\"))\n",
    "print(\"onnx-quantized model size : \", file_size(\"pubmed-quantized.onnx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-diversity",
   "metadata": {},
   "source": [
    "#### Conclusion \n",
    "\n",
    "The best bet in terms of both space and time is onnx quantized model !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-tender",
   "metadata": {},
   "source": [
    "#### Further research\n",
    "1. Knowledge Distillation\n",
    "2. Pruning Attention Heads"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
